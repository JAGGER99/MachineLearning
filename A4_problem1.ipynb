{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "SUi90hUSwzs-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def sklearn_to_df(data_loader):\n",
        "    X_data = data_loader.data\n",
        "    X_columns = data_loader.feature_names\n",
        "    x = pd.DataFrame(X_data, columns=X_columns)\n",
        "\n",
        "    y_data = data_loader.target\n",
        "    y = pd.Series(y_data, name='target')\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Loading a classic binary classification dataset (breast cancer prediction from 30 features).\n",
        "# More details: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
        "x, y = sklearn_to_df(load_breast_cancer())\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    def fit(self, x, y, epochs):\n",
        "\n",
        "        self.weights = np.zeros(x.shape[1]) # 30 weights\n",
        "        self.bias = 0   # a scaler not a vector\n",
        "        self.train_accuracies = []\n",
        "        self.losses = []\n",
        "\n",
        "        for i in range(epochs):\n",
        "\n",
        "            #print(self.weights.shape)\n",
        "            #print(x.transpose().shape)\n",
        "\n",
        "            x_dot_weights = np.matmul(self.weights, x.transpose()) + self.bias\n",
        "            pred = self._sigmoid(x_dot_weights)\n",
        "            loss = self.get_loss(y, pred)\n",
        "            error_w, error_b = self.get_gradients(x, y, pred)\n",
        "            self.update_model_parameters(error_w, error_b)\n",
        "\n",
        "            pred_to_class = [1 if p > 0.5 else 0 for p in pred]\n",
        "            self.train_accuracies.append(accuracy_score(y, pred_to_class))\n",
        "            self.losses.append(loss)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return np.array([self._sigmoid_fn(value) for value in x])\n",
        "\n",
        "    def _sigmoid_fn(self, value):\n",
        "        # @TODO: Implement the (numerically stable) sigmoid function:\n",
        "        if value >= 0:\n",
        "            return (1. / ( 1. + np.exp(-value) ))\n",
        "        else:\n",
        "            return  (np.exp(value) / ( 1. + np.exp(value) ))\n",
        "\n",
        "    def get_loss(self, y_true, y_pred):\n",
        "        # binary cross entropy\n",
        "        # @TODO: Implement the binary cross-entropy loss:\n",
        "        y_one_loss = y_true * np.log(y_pred + 0.000000001)\n",
        "        y_zero_loss = (1 - y_true) * np.log(1 - y_pred + 0.000000001)\n",
        "        return -np.mean(y_zero_loss + y_one_loss)\n",
        "\n",
        "\n",
        "    def get_gradients(self, x, y_true, y_pred):\n",
        "        # derivative of binary cross entropy (i.e. derivative of the loss function)\n",
        "        # @TODO: from y_true and y_pred, compute the bias gradient:\n",
        "        gradient_b = np.sum(y_pred - y_true) * (1 / y_true.shape[0]) # we take the mean of the differences between the prediction and the true label to get a SINGLE VALUE ... not a vector!\n",
        "\n",
        "        # @TODO: from x, y_true, and y_pred, compute the weight gradient:\n",
        "\n",
        "        #print(x.transpose().shape)\n",
        "        #print(y_pred.shape)\n",
        "\n",
        "        gradients_w = np.matmul(x.transpose() , (y_pred - y_true))\n",
        "\n",
        "        return gradients_w, gradient_b\n",
        "\n",
        "    def update_model_parameters(self, error_w, error_b):\n",
        "        learning_rate = 0.1\n",
        "\n",
        "        self.weights = self.weights - learning_rate * error_w\n",
        "        self.bias = self.bias - learning_rate * error_b\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "        #print(x.shape)\n",
        "        #print(self.weights.transpose().shape)\n",
        "\n",
        "        x_dot_weights = np.matmul(x, self.weights.transpose()) + self.bias\n",
        "\n",
        "        probabilities = self._sigmoid(x_dot_weights)\n",
        "        return [1 if p > 0.5 else 0 for p in probabilities]"
      ],
      "metadata": {
        "id": "0zxxEpMUw-5_"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do NOT modify this cell\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(x_train, y_train, epochs=500)    # We train for 500 epochs\n",
        "pred = classifier.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, pred)\n",
        "print(accuracy) # Expected accuracy is >= 0.94\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8SFsVrwxJ8_",
        "outputId": "7fc5addf-d655-42ef-ca6c-1651b49b0d55"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9473684210526315\n"
          ]
        }
      ]
    }
  ]
}